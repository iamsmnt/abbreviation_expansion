{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamsmnt/test1/blob/master/Copy_of_Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Q5DjAJF_1kRh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P46GQ8Mq1zRR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LanguageNgramModel:\n",
        "    \"\"\" \n",
        "    The model remembers and predicts which letters follow which.\n",
        "    Constructor parameters:\n",
        "        order - number of characters the model remembers, or n-1\n",
        "        smoothing - the number, added to each counter for stability\n",
        "        recursive - weight of the model of one order less\n",
        "    Learned parameters:\n",
        "        counter_ - storage of n-grams, as dict of counters  \n",
        "        vocabulary_ - set of characters that the model knows\n",
        "    \"\"\"\n",
        "    def __init__(self, order=1, smoothing=1.0, recursive=0.001):\n",
        "        self.order = order\n",
        "        self.smoothing = smoothing\n",
        "        self.recursive = recursive\n",
        "    \n",
        "    def fit(self, corpus):\n",
        "        \"\"\" Estimate freqency of all n-grams in the text\n",
        "        parameters:\n",
        "            corpus - a text string \n",
        "        \"\"\"\n",
        "        self.counter_ = defaultdict(lambda: Counter())\n",
        "        self.vocabulary_ = set()\n",
        "        for i, token in enumerate(corpus[self.order:]):\n",
        "            context = corpus[i:(i+self.order)]\n",
        "            self.counter_[context][token] += 1\n",
        "            self.vocabulary_.add(token)\n",
        "        self.vocabulary_ = sorted(list(self.vocabulary_))\n",
        "        if self.recursive > 0 and self.order > 0:\n",
        "            self.child_ = LanguageNgramModel(self.order-1, self.smoothing, self.recursive)\n",
        "            self.child_.fit(corpus)\n",
        "       \n",
        "            \n",
        "    def get_counts(self, context):\n",
        "        \"\"\" Estimate frequency of all symbols that may follow the context\n",
        "        Parameters:\n",
        "            context - text string (only the last self.order chars matter)\n",
        "        Returns: \n",
        "            freq - vector of letter conditional frequencies, as pandas.Series\n",
        "        \"\"\"\n",
        "        if self.order:\n",
        "            local = context[-self.order:]\n",
        "      \n",
        "        else:\n",
        "            local = ''\n",
        "        freq_dict = self.counter_[local]\n",
        "        \n",
        "        freq = pd.Series(index=self.vocabulary_)\n",
        "        \n",
        "        for i, token in enumerate(self.vocabulary_):\n",
        "            freq[token] = freq_dict[token] + self.smoothing\n",
        "        if self.recursive > 0 and self.order > 0:\n",
        "            child_freq = self.child_.get_counts(context) * self.recursive\n",
        "            freq += child_freq\n",
        "        return freq\n",
        "    \n",
        "    def predict_proba(self, context):\n",
        "        \"\"\" Estimate probability of all symbols that may follow the context\n",
        "        Parameters:\n",
        "            context - text string (only the last self.order chars matter)\n",
        "        Returns: \n",
        "            freq - vector of letter conditional frequencies, as pandas.Series\n",
        "        \"\"\"\n",
        "        counts = self.get_counts(context)\n",
        "        return counts / counts.sum()\n",
        "    def single_log_proba(self, context, continuation):\n",
        "        \"\"\" Estimate log probability of the certain continuation of the context\n",
        "        Parameters:\n",
        "            context - text string, known beginning of the phrase\n",
        "            continuation - text string, its hypothetical end\n",
        "        Returns: \n",
        "            result - a float, log of probability\n",
        "        \"\"\"\n",
        "        result = 0.0\n",
        "        for token in continuation:\n",
        "            result += np.log(self.predict_proba(context)[token])\n",
        "            context += token\n",
        "        return result\n",
        "    \n",
        "    def single_proba(self, context, continuation):\n",
        "        \"\"\" Estimate probability of the certain continuation of the context\n",
        "        Parameters:\n",
        "            context - text string, known beginning of the phrase\n",
        "            continuation - text string, its hypothetical end\n",
        "        Returns: \n",
        "            result - a float, probability\n",
        "        \"\"\"\n",
        "        return np.exp(self.single_log_proba(context, continuation))\n",
        "            \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQcKQWES_2Li",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_ = LanguageNgramModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JDGCPRt9jU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_.fit('account')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdY2THmO-ktz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f3654e1a-b205-4e34-e564-6aea193b7af0"
      },
      "cell_type": "code",
      "source": [
        "model_.get_counts('c')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "c    2.003\n",
              "n    1.002\n",
              "o    2.002\n",
              "t    1.002\n",
              "u    1.002\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "metadata": {
        "id": "RGwm2Ggd_sqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MissingLetterModel:\n",
        "    \"\"\" \n",
        "    The model remembers and predicts which letters are usually missed.\n",
        "    Constructor parameters:\n",
        "        order - number of characters the model remembers, or n-1\n",
        "        smoothing_missed - the number added to missed counter\n",
        "        smoothing_total - the number added to total counter\n",
        "    Learned parameters:\n",
        "        missed_counter_ - counter of occurences of the missed characters \n",
        "        total_counter_ - counter of occurences of all characters \n",
        "    \"\"\"\n",
        "    def __init__(self, order=0, smoothing_missed=0.3, smoothing_total=1.0):\n",
        "        self.order = order\n",
        "        self.smoothing_missed = smoothing_missed\n",
        "        self.smoothing_total = smoothing_total\n",
        "    \n",
        "    def fit(self, sentence_pairs):\n",
        "        \"\"\" Estimate of missing probability for each symbol\n",
        "        Parameters:\n",
        "            sentence_pairs - list of (original phrase, abbreviation)\n",
        "        In the abbreviation, all missed symbols are replaced with \"-\"\n",
        "        \"\"\"\n",
        "        self.missed_counter_ = defaultdict(lambda: Counter())\n",
        "        self.total_counter_ = defaultdict(lambda: Counter())\n",
        "        for (original, observed) in sentence_pairs:\n",
        "            for i, (original_letter, observed_letter) \\\n",
        "                    in enumerate(zip(original[self.order:], observed[self.order:])):\n",
        "                context = original[i:(i+self.order)]\n",
        "                if observed_letter == '-':\n",
        "                    self.missed_counter_[context][original_letter] += 1\n",
        "                self.total_counter_[context][original_letter] += 1\n",
        "                print(original_letter,observed_letter)\n",
        "    def predict_proba(self, context, last_letter):\n",
        "        \"\"\" Estimate of probability of last_letter being missed after context\"\"\"\n",
        "        if self.order:\n",
        "            local = context[-self.order:]\n",
        "        else:\n",
        "            local = ''\n",
        "        missed_freq = self.missed_counter_[local][last_letter] + self.smoothing_missed\n",
        "        print(missed_freq)\n",
        "        \n",
        "        total_freq = self.total_counter_[local][last_letter] + self.smoothing_total\n",
        "        print(total_freq)\n",
        "        return missed_freq / total_freq\n",
        "    \n",
        "    def single_log_proba(self, context, continuation, actual=None):\n",
        "        \"\"\" Estimate log probability that after context, \n",
        "            continuation is abbreviated to actual.\n",
        "        If actual is None, it is assumed that nothing is abbreviated.\n",
        "        \"\"\"\n",
        "        if not actual:\n",
        "            actual = continuation\n",
        "        result = 0.0\n",
        "        for orig_token, act_token in zip(continuation, actual):\n",
        "            pp = self.predict_proba(context, orig_token)\n",
        "            if act_token != '-':\n",
        "                pp = 1 - pp\n",
        "            result += np.log(pp)\n",
        "            context += orig_token\n",
        "        return result\n",
        "    def single_proba(self, context, continuation, actual=None):\n",
        "        \"\"\" Estimate probability that after context, \n",
        "            continuation is abbreviated to actual.\n",
        "        If actual is None, it is assumed that nothing is abbreviated.\n",
        "        \"\"\"\n",
        "        return np.exp(self.single_log_proba(context, continuation, actual))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFXnppdYOzFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e8e2cb27-a621-4ee7-c478-d15ce78f2760"
      },
      "cell_type": "code",
      "source": [
        "missed_model = MissingLetterModel(0)\n",
        "missed_model.fit([('abracadabra', 'abr-c-d-br-')]) "
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a a\n",
            "b b\n",
            "r r\n",
            "a -\n",
            "c c\n",
            "a -\n",
            "d d\n",
            "a -\n",
            "b b\n",
            "r r\n",
            "a -\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AF9XpKMCS_GQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}